{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "interpreter": {
   "hash": "0adcc2737ebf6a4a119f135174df96668767fca1ef1112612db5ecadf2b6d608"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 신경망 모델 구성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "source": [
    "## 학습을 위한 장치 얻기"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))"
   ]
  },
  {
   "source": [
    "## 클래스 정의하기\n",
    "- 신경망 모델을 `nn.Module`의 하위클래스로 정의\n",
    "- `__init__`에서 신경망 계층들을 초기화\n",
    "- `nn.Module`을 상속받은 모든 클래스는 `forward` 메소드에서, 입력 데이터에 대한 연산을 구현 "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NueralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NueralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten() # 이미지를 1차원으로 펼쳐줌\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(32*32, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n"
   ]
  },
  {
   "source": [
    "- 모델 구조 출력"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "NueralNetwork(\n  (flatten): Flatten(start_dim=1, end_dim=-1)\n  (linear_relu_stack): Sequential(\n    (0): Linear(in_features=1024, out_features=512, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=512, out_features=512, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=512, out_features=10, bias=True)\n    (5): ReLU()\n  )\n)\n"
     ]
    }
   ],
   "source": [
    "model = NueralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Predicted class: tensor([8, 8, 5])\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(3, 32, 32, device=device)\n",
    "logits = model(X)\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"Predicted class: {y_pred}\")"
   ]
  },
  {
   "source": [
    "## 기본적인 합성곱 신경망 구성 방법\n",
    " - `torch.nn.conv2d` 참고 링크 => [홈페이지](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Net(\n  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=same)\n  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n  (dropout1): Dropout2d(p=0.25, inplace=False)\n  (dropout2): Dropout2d(p=0.5, inplace=False)\n  (fc1): Linear(in_features=9216, out_features=128, bias=True)\n  (fc2): Linear(in_features=128, out_features=10, bias=True)\n)\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "      super(Net, self).__init__()\n",
    "\n",
    "      # 첫번째 2D 합성곱 계층\n",
    "      # 1개의 입력 채널(이미지)을 받아들이고, 사각 커널 사이즈가 3인 32개의 합성곱 특징들을 출력합니다.\n",
    "      # (in_channels, out_channels, kernel_size, stride, padding='valid', 'same' or int)\n",
    "      self.conv1 = nn.Conv2d(1, 32, 3, 1, 'same')\n",
    "      # 두번째 2D 합성곱 계층\n",
    "      # 32개의 입력 게층을 받아들이고, 사각 커널 사이즈가 3인 64개의 합성곱 특징을 출력합니다.\n",
    "      self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "\n",
    "      # 인접한 픽셀들은 입력 확률에 따라 모두 0 값을 가지거나 혹은 모두 유효한 값이 되도록 만듭니다.\n",
    "      self.dropout1 = nn.Dropout2d(0.25)\n",
    "      self.dropout2 = nn.Dropout2d(0.5)\n",
    "\n",
    "      # 첫번째 fully connected layer\n",
    "      self.fc1 = nn.Linear(9216, 128)\n",
    "      # 10개의 라벨을 출력하는 두번째 fully connected layer\n",
    "      self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "my_nn = Net()\n",
    "print(my_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "      super(Net, self).__init__()\n",
    "      self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "      self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "      self.dropout1 = nn.Dropout2d(0.25)\n",
    "      self.dropout2 = nn.Dropout2d(0.5)\n",
    "      self.fc1 = nn.Linear(9216, 128)\n",
    "      self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    # x는 데이터를 나타냅니다.\n",
    "    def forward(self, x):\n",
    "      # 데이터가 conv1을 지나갑니다.\n",
    "      x = self.conv1(x)\n",
    "      # x를 ReLU 활성함수(rectified-linear activation function)에 대입합니다.\n",
    "      x = F.relu(x)\n",
    "\n",
    "      x = self.conv2(x)\n",
    "      x = F.relu(x)\n",
    "\n",
    "      # x에 대해서 max pooling을 실행합니다.\n",
    "      x = F.max_pool2d(x, 2)\n",
    "      # 데이터가 dropout1을 지나갑니다.\n",
    "      x = self.dropout1(x)\n",
    "      # start_dim=1으로 x를 압축합니다.\n",
    "      x = torch.flatten(x, 1)\n",
    "      # 데이터가 fc1을 지나갑니다.\n",
    "      x = self.fc1(x)\n",
    "      x = F.relu(x)\n",
    "      x = self.dropout2(x)\n",
    "      x = self.fc2(x)\n",
    "\n",
    "      # x에 softmax를 적용합니다.\n",
    "      output = F.log_softmax(x, dim=1)\n",
    "      return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[-2.4453, -2.3968, -2.3191, -2.3904, -2.1992, -2.3121, -2.1617, -2.3920,\n",
      "         -2.2905, -2.1664]], grad_fn=<LogSoftmaxBackward>)\n",
      "/usr/local/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ../c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# 임의의 28x28 이미지로 맞춰줍니다.\n",
    "random_data = torch.rand((1, 1, 28, 28))\n",
    "\n",
    "my_nn = Net()\n",
    "result = my_nn(random_data)\n",
    "print (result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}