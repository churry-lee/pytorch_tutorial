{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "interpreter": {
   "hash": "0adcc2737ebf6a4a119f135174df96668767fca1ef1112612db5ecadf2b6d608"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, models, transforms\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "source": [
    "# PYTORCH CHEAT SHEET"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Imports"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### General"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch                                        # root package\n",
    "from torch.utils.data import Dataset, DataLoader    # dataset representation and loading"
   ]
  },
  {
   "source": [
    "### Neural Network API\n",
    "- [nn](https://pytorch.org/docs/stable/nn.html)\n",
    "- [functional](https://pytorch.org/docs/stable/nn.html#torch-nn-functional)\n",
    "- [optim](https://pytorch.org/docs/stable/optim.html)\n",
    "- [autograd](https://pytorch.org/docs/stable/autograd.html)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.nn.functional as F           # layers, activations and more\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "from torch.jit import script, trace       # hybrid frontend decorator and tracing jit"
   ]
  },
  {
   "source": [
    "### [Vision](https://pytorch.org/vision/stable/index.html)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, models, transforms     # vision datasets,\n",
    "                                                         # architectures &\n",
    "                                                         # transforms\n",
    "\n",
    "import torchvision.transforms as transforms              # composable transforms"
   ]
  },
  {
   "source": [
    "### Distributed Training(분산 학습)\n",
    "- [Distributed](https://pytorch.org/docs/stable/distributed.html)\n",
    "- [Multiprocessing](https://pytorch.org/docs/stable/multiprocessing.html)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.distributed as dist             # distributed communication\n",
    "from torch.multiprocessing import Process    # memory sharing processes"
   ]
  },
  {
   "source": [
    "## Tensors"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Creation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(*size)              # tensor with independent N(0,1) entries\n",
    "x = torch.[ones|zeros](*size)       # tensor with all 1's [or 0's]\n",
    "x = torch.tensor(L)                 # create tensor from [nested] list or ndarray L\n",
    "y = x.clone()                       # clone of x\n",
    "with torch.no_grad():               # code wrap that stops autograd from tracking tensor history\n",
    "requires_grad=True                  # arg, when set to True, tracks computation\n",
    "                                    # history for future derivative calculations"
   ]
  },
  {
   "source": [
    "### Dimensionality"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.size()                                  # return tuple-like object of dimensions\n",
    "x = torch.cat(tensor_seq, dim=0)          # concatenates tensors along dim\n",
    "y = x.view(a,b,...)                       # reshapes x into size (a,b,...)\n",
    "y = x.view(-1,a)                          # reshapes x into size (b,a) for some b\n",
    "y = x.transpose(a,b)                      # swaps dimensions a and b\n",
    "y = x.permute(*dims)                      # permutes dimensions\n",
    "y = x.unsqueeze(dim)                      # tensor with added axis\n",
    "y = x.unsqueeze(dim=2)                    # (a,b,c) tensor -> (a,b,1,c) tensor\n",
    "y = x.squeeze()                           # removes all dimensions of size 1 (a,1,b,1) -> (a,b)\n",
    "y = x.squeeze(dim=1)                      # removes specified dimension of size 1 (a,1,b,1) -> (a,b,1)"
   ]
  },
  {
   "source": [
    "### Algebra"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = A.mm(B)       # matrix multiplication\n",
    "ret = A.mv(x)       # matrix-vector multiplication\n",
    "x = x.t()           # matrix transpose"
   ]
  },
  {
   "source": [
    "### GPU Usage"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available                                     # check for cuda\n",
    "x = x.cuda()                                                # move x's data from\n",
    "                                                            # CPU to GPU and return new object\n",
    "\n",
    "x = x.cpu()                                                 # move x's data from GPU to CPU\n",
    "                                                            # and return new object\n",
    "\n",
    "if not args.disable_cuda and torch.cuda.is_available():     # device agnostic code\n",
    "    args.device = torch.device('cuda')                      # and modularity\n",
    "else:                                                       #\n",
    "    args.device = torch.device('cpu')                       #\n",
    "\n",
    "net.to(device)                                              # recursively convert their\n",
    "                                                            # parameters and buffers to\n",
    "                                                            # device specific tensors\n",
    "\n",
    "x = x.to(device)                                            # copy your tensors to a device\n",
    "                                                            # (gpu, cpu)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "source": [
    "### [Deep Learning](https://pytorch.org/docs/stable/nn.html)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Linear(m,n)                                # fully connected layer from\n",
    "                                              # m to n units\n",
    "\n",
    "nn.ConvXd(m,n,s)                              # X dimensional conv layer from\n",
    "                                              # m to n channels where X⍷{1,2,3}\n",
    "                                              # and the kernel size is s\n",
    "\n",
    "nn.MaxPoolXd(s)                               # X dimension pooling layer\n",
    "                                              # (notation as above)\n",
    "\n",
    "nn.BatchNormXd                                # batch norm layer\n",
    "nn.RNN/LSTM/GRU                               # recurrent layers\n",
    "nn.Dropout(p=0.5, inplace=False)              # dropout layer for any dimensional input\n",
    "nn.Dropout2d(p=0.5, inplace=False)            # 2-dimensional channel-wise dropout\n",
    "nn.Embedding(num_embeddings, embedding_dim)   # (tensor-wise) mapping from\n",
    "                                              # indices to embedding vectors"
   ]
  },
  {
   "source": [
    "### [Loss Functions](https://pytorch.org/docs/stable/nn.html#loss-functions)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.X                                  # where X is L1Loss, MSELoss, CrossEntropyLoss\n",
    "                                      # CTCLoss, NLLLoss, PoissonNLLLoss,\n",
    "                                      # KLDivLoss, BCELoss, BCEWithLogitsLoss,\n",
    "                                      # MarginRankingLoss, HingeEmbeddingLoss,\n",
    "                                      # MultiLabelMarginLoss, SmoothL1Loss,\n",
    "                                      # SoftMarginLoss, MultiLabelSoftMarginLoss,\n",
    "                                      # CosineEmbeddingLoss, MultiMarginLoss,\n",
    "                                      # or TripletMarginLoss"
   ]
  },
  {
   "source": [
    "### [Activation Functions](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.X                                  # where X is ReLU, ReLU6, ELU, SELU, PReLU, LeakyReLU,\n",
    "                                      # RReLu, CELU, GELU, Threshold, Hardshrink, HardTanh,\n",
    "                                      # Sigmoid, LogSigmoid, Softplus, SoftShrink,\n",
    "                                      # Softsign, Tanh, TanhShrink, Softmin, Softmax,\n",
    "                                      # Softmax2d, LogSoftmax or AdaptiveSoftmaxWithLoss"
   ]
  },
  {
   "source": [
    "### [Optimizers](https://pytorch.org/docs/stable/optim.html)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.x(model.parameters(), ...)      # create optimizer\n",
    "opt.step()                                  # update weights\n",
    "optim.X                                     # where X is SGD, Adadelta, Adagrad, Adam,\n",
    "                                            # AdamW, SparseAdam, Adamax, ASGD,\n",
    "                                            # LBFGS, RMSprop or Rprop"
   ]
  },
  {
   "source": [
    "### [Learning rate scheduling](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = optim.X(optimizer,...)      # create lr scheduler\n",
    "scheduler.step()                        # update lr at start of epoch\n",
    "optim.lr_scheduler.X                    # where X is LambdaLR, MultiplicativeLR,\n",
    "                                        # StepLR, MultiStepLR, ExponentialLR,\n",
    "                                        # CosineAnnealingLR, ReduceLROnPlateau, CyclicLR,\n",
    "                                        # OneCycleLR, CosineAnnealingWarmRestarts,"
   ]
  },
  {
   "source": [
    "### [Datasets]"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = datasets.CIFAR10(root='./data', train=True, download=True, transform=pre_processing)\n",
    "test_set = datasets.CIFAR10(root='./data', train=False, download=True, transform=pre_processing)"
   ]
  },
  {
   "source": [
    "### [Dataloaders and DataSamplers](https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_data, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True, num_workers=2)"
   ]
  }
 ]
}